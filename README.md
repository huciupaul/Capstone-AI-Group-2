# Precursor identification of extreme events in turbulence
This project aims to identify precursor flow states to extreme events using the Kolmogorov flow as a proof of concept. The codebase consists of three main components. Namely, 
- Data Generation of snapshots of a velocity vector space
- Multiscale Convolutional Autoencoder (CAE) to reduce the dimensionality of the data
- Modularity-Based Clustering algorithm to identify precursor clusters.

The codebase is written in Python, mainly using the TensorFlow library. This project is part of TU Delft course TI3165TU - Capstone Applied AI project 2024/25, and is supervised by Nguyen Anh Khoa Doan.

## Table of Contents
- [Environment Requirements](#environment-requirements)
- [Data Generation](#data-generation)
  - [Usage for Data Generation](#usage---data-generation)
- [Multiscale Convolutional Autoencoder (CAE)](#multiscale-convolutional-autoencoder-cae)
  - [Architecture](#architecture)
  - [Data Split](#data-split)
  - [Training and Hyperparameter Tuning](#training-and-hyperparameter-tuning)
  - [Testing](#testing)
  - [Encoding and Decoding](#encoding-and-decoding)
  - [Usage for CAE](#usage-for-cae)
- [Clustering](#clustering)
  - [Implementation](#implementation)
  - [Features](#features)
  - [Post-Processing](#post-processing)
  - [Usage for Clustering](#usage-for-clustering)
  - [Alternative Clustering Methods](#alternative-clustering-methods)
    - [K-Means Clustering](#k-means-clustering)
    - [Agglomerative Clustering](#agglomerative-clustering)
    - [DBSCAN Clustering](#dbscan-clustering)
  - [Visualization](#visualisation)
- [Acknowledgments](#acknowledgments)


### Environment Requirements
This code is tested on TensorFlow version 2.10 and python 3.9. The required packages for the data generation and the CAE can be installed with pip using:

```bash
pip install -r requirements.txt
```
Or for anaconda users with:

```bash 
conda env create -f environment.yaml
```

For clustering, the relevant depedencies can be found at `clustering/requirements_clustering.txt`. 
This can be installed with:

```bash
pip install -r 'requirements_clustering.txt'
```


## Data Generation
The data generation is done using the KolSol python library, which is a Kolmogorov flow solver. The Kolmogorov flow is a two-dimensional flow which has a periodic function applied, commonly used to simulate simple turbulent flow. The solver provides numerical solutions to the divergence-free Navier-Stokes equations:

$$
\frac{\partial \mathbf{u}}{\partial t} + \mathbf{u} \cdot \nabla \mathbf{u} = -\nabla p + \frac{1}{Re} \Delta \mathbf{u} + \mathbf{f},
$$

$$
\nabla \cdot \mathbf{u} = 0,
$$

Since the analysis is done for turbulent flow, the Reynolds number is set to 40, the threshold for turbulence in the Kolmogorov flow. Before generating the actual data, there is a run of 200 transient steps where the flow model stabilizes.

For the actual generation, a timestep of 0.01 s is used, and the data is saved every 20 steps, leading to dt = 0.2s, to maintain diversity in the dataset while retaining enough sequential information. During the generation loop, the flow field is generated by the library, whereas the dissipation is computed using the funtion `dissip` and the kinetic energy calculation is implemented manually for every generated step. 

After the generation the data is saved as a H5 file for further analysis. Besides, the disipation rate and kinetic evergy plots are generated for visualization.

**With this, the requirement M1 is fulfilled: Must receive 2D Kolmogorov flow state as time series data, formatted as a Python tensor of shape [δt x H x W x 2], where δt is the number of time steps and H and W are the grid dimensions, where each point is of 2 velocity components.**

_Note: where applicable in this project, a random seed of 42 is used_

### Usage for Data Generation
To generate data, run `data_generator.py`, the produced generated file (ex.: `Data_Generated.h5`) should be copied into the CAE data folder ( `.\CAE\Data\`).

## Multiscale Convolutional Autoencoder
The convolutional autoencoder (CAE) codebase consists of multiple python scripts. `cae_main.py` is the main file that loads data, creates the encoder and decoder modules, trains the CAE and performs validation. It calls functions from `autoencoder.py`, `constants.py`, `helpers.py`, `metrics.py`, `prepare_data.py`, `train.py` and `visualization.py`. Constants can be altered in `constants.py` to alter the specifics of the model architecture.

Hyperparameter tuning is performed in `hyperparameter_tuning.py` and the output is saved to `hyperparameter_tuning.txt`. Then, for encoding and decoding the data `encode_general.py` and `decode_general.py` are for general usage. For our specific use case, we have prepared the data for modularity based clustering in `encode_clustering.py` and `decode_clustering.py`.

### Architecture
The CAE consists of an encoder and a decoder, as implemented in ["Predicting turbulent dynamics with the convolutional autoencoder echo state network"](https://www.cambridge.org/core/journals/journal-of-fluid-mechanics/article/predicting-turbulent-dynamics-with-the-convolutional-autoencoder-echo-state-network/1E0F75CD94FCB3A1354A09622F8D25CD), which was inspired by the work of [Hasegawa (2020)](https://doi.org/10.1007/s00162-020-00528-w). The CAE architecture is defined in `autoencoder.py`. It is a multiscale autoencoder, with `N_parallel=3`, meaning there are 3 encoder modules and 3 decoder modules, with a designated kernel size each, enabling the model to capture various spatial relations. The latent variables and decoded output are obtained by summing up the outputs of the encoder modules and decoder modules respectively. Each encoder module consists of `N_layers=4` convolutional layers. This is followed by our own implementation of 2 fully connected layers to further reduce the dimension of the data to the desired parameter `N_lat`. Periodic padding ensures consistent spatial dimensions during convolutions, preserving boundary information. All the layers use `tanh` activation functions. The decoder architecture is the mirror image of the encoder.

### Data Split

The data is split sequentially into 80% training, 10% validation, and 10% tuning to preserve the natural order of the turbulence flow states. This prevents data leakage, ensuring the model does not learn from future states and generalizes effectively to unseen data.

Batching was done separately for the training, validation and test dataset. For each dataset, a batch is formed by taking one snapshot every `batch_size` snapshot, ensuring the sequence of the data is removed. This helps with maintaining diversity in the batches and discouraging the model from learning sequential patterns so that it generalizes well.

The original training set used has 24 000 samples, but this was reduced in the repository for easy testing.

### Training and hyperparameter tuning
In `train.py` the CAE model is trained on the snapshots of the velocity vector space. At its core, it iteratively minimizes the reconstruction error (Mean Squared Error) between the input data and the CAE's output using the Adam optimizer.  The loop incorporates several features, including:

* Learning Rate Adaptation: The learning rate decreases dynamically by a factor `lrate_mult` when the training loss increases, enhancing convergence stability.
* Early Stopping: Training stops if the validation loss does not improve for a predefined `patience` number of epochs, preventing overfitting and saving computational resources.
* Validation Monitoring: The validation loss is computed every `N_check` epochs to evaluate generalization performance.
* Checkpointing: The best-performing model weights and optimizer parameters are saved based on the lowest validation loss. If the training loss does not improve compared to the average of `N_lr` previous epochs, the learning rate is adapted and the model is reset to a previous best-performing state.
* Visualization: Loss plots are saved every `N_plot` epochs to visualize training and validation trends.

For hyperparameter tuning, the `hyperparameter_tuning.py` script trains models over a set of latent space, `n_lat` dimensions and saves the results to `hyperparameter_tuning.txt`. The optimal `n_lat` was selected through a manual trade-off between NRMSE and its size.

**With this, the (revised) M2 requirement is fulfilled: Must encode the 2D flow state into a latent space with a dimen-
sionality of at most 12, as fixed by the bottleneck layer of the AE architecture.**


### Testing
In `model_test.py` the convolutional autoencoder model's performance is tested. The testing dataset is loaded to `U_test`, only calling the samples at timesteps after training and validation. Then, it is batched and passed through the model. The predicted values are then compared to the actual test values using an NRMSE metric.

The loop for each epoch contains the loading of encoder and decoder using the `load_encoder` and `load_decoder` functions from `helpers.py`, the NRMSE calculation is done using `compute_nrmse` from `metrics.py`. Cross validation was not performed due to the large computation needed. This was balanced by using a fairly large test set, ensuring confidence in the results.

**Thus, the (revised) requirement M3, "Must reconstruct the full flow feature from the latent space with a reconstruction error (normalized root-mean-squared error, NRMSE) below 25%," is fulfilled.**

Furthermore, the verification approach was changed from ensuring that ALL snapshots have an NRMSE below the threshold to ensuring that the AVERAGE NRMSE is below the threshold. This was adjusted to account for outliers.


### Encoding and decoding
The encoding and decoding of the data are done using the two functions from `encode_general.py` and `decode_general.py`. The encoder part uses the pre-trained model to encode the data to the set latent space. The decoder loads the encoded data and decodes it back to its original shape.

During encoding, the model is loaded and the data is batched according to the preset `batch_size` and `n_batches`. Subsequently, the data is forward passed through the loaded encoder and the encoded data is saved to a HDF5 file.

The decoding is the reverse process of encoding, which loads the pre-trained model, takes the encoded data, batches it, decodes it to the original dimension and saves it. Both encoded and decoded data are saved in an unbatched format for general use.

### Usage for CAE
Run the files as follows according to the intended process.
1. To train the model, run `cae_main.py`.
2. To perform hyperparameter tuning, run  `hyperparameter_tuning.py`.
3. To test the model, run `model_test.py`
4. To encode velocity fields, run `encode_general.py`
5. To decode the latent variables, run `decode_general.py`


## Modularity-based Clustering
Modularity-based clutering is employed due to its ability in handling time-series data. This implementation consists of six files in total. Main file is `main_with_loop_only_features.py` which uses functions defined in `clustering_func_only_features.py`, `modularity.py`, `spectralopt.py` and `_divide.py`. In the main file, after the clustering process is done, the clusters are saved to .npz files. These clusters then can be used in `main_load_clusters.py` which postprocesses, calculates average time between extreme and precursor events, detects false positives and negatives and plots phase space plot, tesselated phase space plot and Dissipation time series with background color plot, **thus completing requirements M4 and S1**. `main_load_clusters.py` also saves the precursor centroids that have been validated to lead to an extreme event as `Precursor_Centroids.h5` for the decoder.

### Implementation
The clustering implementation is distributed across multiple scripts:
- **`main_with_loop_only_features.py`**: Main script for the modularity-based clustering process, utilizing functions from supporting scripts.
- **`clustering_func_only_features.py`**: Contains helper functions for clustering, including graph transformations and calculations related to extreme events.
- **`modularity.py`**: Implements core modularity calculations and modularity matrix computations.
- **`spectralopt.py`**: Provides functions for spectral methods used to partition the network into communities.
- **`_divide.py`**: Implements the division of communities into subgroups based on modularity optimization.

The clustering algorithm takes preprocessed velocity field data as input, applies modularity maximization, and saves the resulting clusters as `.npz` files for further analysis.

---

### Features
Key features of the clustering implementation include:
1. **Spectral Clustering**: Uses spectral methods to partition the network into communities. The largest eigenvalues and eigenvectors of the modularity matrix are computed, which identifies optimal divisions of the network. This process iteratively maximizes the modularity metric.
2. **Refinement**: The clustering process includes refinement steps to enhance the modularity and ensure well-defined community structures. The algorithm avoids over-segmentation by stopping further division when the modularity metric no longer improves. 
3. **Extreme and Precursor Cluster Identification**: The clustering framework distinguishes between normal, precursor and extreme clusters.
4. **Visualization**: Results are visualized as:
   - **Phase space plots** that display the distribution of data points and their transitions between clusters. 
   - **Tessellated phase space plots** which highlights the tessellation of phase space into discrete clusters.
   - **Dissipation time series with color-coded clusters** - overlay time series with color-coded clusters, allowing for an intuitivie understanding of how extreme events and their precursors evolve over time.
5. **Path and Transition Analysis**:
   - The algorithm computes paths between clusters, particularly focusing on transitions from precursor clusters to extreme clusters.
   - It calculates metrics such as:
     - **Maximum Transition Probability**: The likelihood of transitioning from one cluster to another.
     - **Shortest Path**: The minimum number of transitions needed to reach an extreme cluster.
     - **Average Transition Time**: The mean time taken to transition between clusters.

---

### Post-Processing
Post-processing is handled in **`main_load_clusters.py`**, which:
- Computes the average time between precursor and extreme events.
- Identifies false positives and negatives in precursor detection.
- Generates detailed visualizations to understand the dynamics of the identified clusters.

Additionally, `physical_interpretation.py` from the CAE folder decodes latent space variables, inlcuding the centroids of normal, precursor and extreme clusters, **in accordance with requirement C1**.

---

### Usage for Clustering
To run the clustering process:
1. In the CAE folder run `encode_clustering.py` to create the encoded data file for clustering (ex.: `48_Encoded_data_Re40_10.h5`). Ensure the paths to the model folder and generated data file are correct before running.
   a. The encoded data file (ex.: `48_Encoded_data_Re40_10.h5`) from CAE Data folder (`.\CAE\Data\`) should be copied into the modularity based clustering data folder (`.\Modularity Based Clustering\Data\`)
2. Ensure the dependencies listed in `clustering/requirements_clustering.txt` are installed.
3. Ensure that file paths are defined correctly for your dataset in **`main_with_loop_only_features.py`** and **`main_load_clusters.py`**.
4. Use **`main_with_loop_only_features.py`** to execute the clustering pipeline. 
5. Analyze the saved clusters with **`main_load_clusters.py`** for further insights and to save the precursor cluster centroids (Cluster centroids saved in `.\Modularity Based Clustering\Data\` as `Cluster_Centroids.h5`)
6. To decode the `Cluster_Centroids.h5`, return to the CAE folder and run file `decode_clustering.py`. 
   a. Before running, place a copy of `Cluster_Centroids.h5` in the CAE data folder (`.\CAE\Data\`) 
7. To visualize the decoded precursor cluster plot, run `physical_interpretation.py` from CAE

### Remark
The file `main_with_loop_only_features.py` encounters a value error (ex: 'ValueError: 1411845100 is not in list') when running in a traditional IDE. Running in a virtual linux enviroment (we used Ubuntu terminal) resolves this error.
---


### Alternative clustering methods
To compare the modularity based clustering with other, more common clustering methods, this repository implements three alternative clustering methods, 'K-Means', 'Agglomerative clustering' and 'DBSCAN'. Which can be found in `clustering/Alternative Clustering/Alternative clusters.py`, `clustering/Alternative Clustering/Alternative clusters agglomerative.py`, `clustering/Alternative Clustering/Alternative clusters density.py` respectively.
These clustering methods are applied to a normalized and reduced subset to reduce computing times. **These alternative clustering methods complete requirement C2**.

## K-Means Clustering
- Partitions data into a user-defined number of clusters (k)
- Identifies extreme events by detecting points deviating significantly from the mean (±5 standard deviations)

## Agglomerative Clustering
- A hierarchical clustering method that partitions data into a user-defined number of clusters with the 'ward linkage' approach.
- Groups data into clusters based on similarity
- Identifies extreme events by detecting points deviating significantly from the mean (±5 standard deviations)

## DBSCAN Clustering
- Density-based clustering method that groups points based on local density
- Treats outliers as noise.
- Identifies extreme events by detecting points deviating significantly from the mean (±5 standard deviations)

## Visualization
Each file plots the clusters found, as well as the extreme events in a 2D scatter plot, showing the x_extreme against the x_0 value.


## Acknowledgments
We would like to express our gratitude to those who contributed to the success of this project.

We acknowledge the [foundational work for the autoencoder](https://github.com/MagriLab/CAE-ESN-Kolmogorov/blob/main/tutorial/CAE-ESN.ipynb) by Alberto Racca, upon which the autoencoder code is adapted. We also drew inspiration from the accompanied research paper ["Predicting turbulent dynamics with the convolutional autoencoder echo state network"](https://www.cambridge.org/core/journals/journal-of-fluid-mechanics/article/predicting-turbulent-dynamics-with-the-convolutional-autoencoder-echo-state-network/1E0F75CD94FCB3A1354A09622F8D25CD) authored by Alberto Racca, Nguyen Anh Khoa Doan, and Luca Magri. For the modularity-based clustering, ["Clustering-based Identification of Precursors of Extreme Events in Chaotic Systems"](https://arxiv.org/abs/2306.16291) by Urszula Golyska and Nguyen Anh Khoa Doan was used as a reference.

Special thanks to our supervisor, Nguyen Anh Khoa Doan, for providing guidance and invaluable insights throughout the project. We would also like to thank Alexandra Neagu, for her support and assistance. Their contributions and expertise were instrumental in the completion of this work.
