{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef3b59b4",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfee64004020ddf8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T20:26:51.485782800Z",
     "start_time": "2025-01-14T20:26:51.474909300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\huciu\\anaconda3\\envs\\precursor\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abdd192",
   "metadata": {},
   "source": [
    "Number of threads to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T20:27:30.595583600Z",
     "start_time": "2025-01-14T20:27:30.579528600Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = '15' # set cores for numpy\n",
    "os.environ['TF_INTER_OP_PARALLELISM_THREADS'] = '15' # set cores for TF\n",
    "os.environ['TF_INTRA_OP_PARALLELISM_THREADS'] = '15'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df9a9c3",
   "metadata": {},
   "source": [
    "Configure TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60faf1bcc66cef9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T20:27:31.519941800Z",
     "start_time": "2025-01-14T20:27:31.457307900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')  # Suppress TensorFlow logs\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b7d715",
   "metadata": {},
   "source": [
    "Remaining imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a2bbddf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T20:27:33.028884400Z",
     "start_time": "2025-01-14T20:27:32.183983700Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import h5py\n",
    "import time\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927fe4f2d90758e3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bf3de97e6910cb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ceda566dd0d31a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T20:27:34.519570600Z",
     "start_time": "2025-01-14T20:27:34.487544Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of the HDF5 file: ['dissipation_rates', 'kinetic_energy', 'velocity_field']\n"
     ]
    }
   ],
   "source": [
    "# Data is generated from Gen_data.ipynb. \n",
    "# ATTENTION: Get the data from Teams! File is too large to upload to GitHub\n",
    "# Note: change fln to the location in your computer, don't need to upload to your IDE\n",
    "# Note: add 'r' in front of the string to escape '\\'\n",
    "fln = r\"C:\\Users\\huciu\\OneDrive - Delft University of Technology\\Study\\Year_3\\Capstone_project\\Generated_data\\data_12000_steps\\Generated_data.h5\"\n",
    "hf  = h5py.File(fln,'r')\n",
    "\n",
    "# Check the contents of the HDF5 file\n",
    "print(\"Contents of the HDF5 file:\", list(hf.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673ed6a0",
   "metadata": {},
   "source": [
    "Initializing the velocity field "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fad809402ca51b84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T20:31:30.979737800Z",
     "start_time": "2025-01-14T20:27:39.959587300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the velocity field: (2400, 48, 48, 2)\n",
      "So, the velocity field has 2400 samples, each sample has 48x48 grid points and each grid point has 2 components (ux,uy)\n"
     ]
    }
   ],
   "source": [
    "downsample = 5 # Skip every n-th sample to get varying data. Otherwise the data is too similar. Set to 1 to use all data.\n",
    "Re = 40\n",
    "data_len = 12000\n",
    "transient = 200 # Number of transient steps to ignore from our data\n",
    "\n",
    "# Load the velocity field\n",
    "U = np.array(hf.get('velocity_field')[transient:transient+data_len:downsample], dtype=np.float32)\n",
    "total_samples = U.shape[0]\n",
    "hf.close()\n",
    "\n",
    "# Check the length of the velocity field\n",
    "print(\"Shape of the velocity field:\", U.shape)  \n",
    "print(f\"So, the velocity field has {total_samples} samples, each sample has 48x48 grid points and each grid point has 2 components (ux,uy)\")\n",
    "\n",
    "# Define the number of grid points in x and y directions\n",
    "N_x     = U.shape[1] \n",
    "N_y     = U.shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207c6d5934ac3579",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Splitting the data into train, validation and test sets, and batching the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b23101c0562d8ab9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T20:35:36.199415800Z",
     "start_time": "2025-01-14T20:35:35.866337800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches: 60, Train batches: 40, Validation batches: 10, Test batches: 10\n"
     ]
    }
   ],
   "source": [
    "b_size = 40   #batch_size\n",
    "n_batches = total_samples//b_size  #number of batches\n",
    "\n",
    "#Split the data into training, validation and test sets with 70%, 15% and 15% of the data respectively\n",
    "train_batches = int(n_batches*(4/6))\n",
    "val_batches = int(n_batches*(1/6))\n",
    "test_batches = int(n_batches*(1/6))\n",
    "\n",
    "# train_batches = 400\n",
    "# val_batches = 99\n",
    "# test_batches = 100\n",
    "\n",
    "print(f\"Number of batches: {n_batches}, Train batches: {train_batches}, Validation batches: {val_batches}, Test batches: {test_batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a22f0e305d98448",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T20:36:02.212748800Z",
     "start_time": "2025-01-14T20:36:02.178063600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def batch_data(U, b_size, n_batches):\n",
    "    \n",
    "    '''\n",
    "    Splits the data in batches. Each batch is created by sampling the signal with interval\n",
    "    equal to n_batches\n",
    "    '''\n",
    "    data = np.zeros((n_batches, b_size, U.shape[1], U.shape[2], U.shape[3]))    \n",
    "    for i in range(n_batches):\n",
    "        data[i] = U[i::n_batches].copy()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "890776f3731ddf5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T20:36:08.587989400Z",
     "start_time": "2025-01-14T20:36:04.648875300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the training data before batching: (1600, 48, 48, 2)\n",
      "Shape of the training data after batching: (40, 40, 48, 48, 2) \n",
      "\n",
      "Shape of the validation data before batching: (400, 48, 48, 2)\n",
      "Shape of the validation data after batching: (10, 40, 48, 48, 2) \n",
      "\n",
      "Shape of the test data before batching: (400, 48, 48, 2)\n",
      "Shape of the test data after batching: (10, 40, 48, 48, 2)\n"
     ]
    }
   ],
   "source": [
    "# training data\n",
    "U_train_unbatched = np.array(U[:train_batches*b_size].copy())\n",
    "print(\"Shape of the training data before batching:\", U_train_unbatched.shape)\n",
    "U_train = batch_data(U_train_unbatched, b_size, train_batches).astype('float32') # to be used for randomly shuffled batches\n",
    "print(\"Shape of the training data after batching:\", U_train.shape, \"\\n\")\n",
    "\n",
    "# validation data\n",
    "U_val_unbatched = np.array(U[train_batches*b_size:train_batches*b_size+b_size*val_batches].copy())\n",
    "print(\"Shape of the validation data before batching:\", U_val_unbatched.shape)\n",
    "U_val = batch_data(U_val_unbatched, b_size, val_batches).astype('float32')         \n",
    "print(\"Shape of the validation data after batching:\", U_val.shape, \"\\n\")\n",
    "\n",
    "# test data\n",
    "U_test_unbatched = np.array(U[train_batches*b_size+b_size*val_batches:].copy())\n",
    "print(\"Shape of the test data before batching:\", U_test_unbatched.shape)\n",
    "U_test = batch_data(U_test_unbatched, b_size, test_batches).astype('float32')\n",
    "print(\"Shape of the test data after batching:\", U_test.shape)\n",
    "\n",
    "# Delete the original data to save memory:\n",
    "del U_train_unbatched, U_val_unbatched"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8879234b2b19161c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Autoencoder Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24e5f94935abd91d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T20:41:48.293871200Z",
     "start_time": "2025-01-14T20:41:48.263672600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Changes: included loss mse and optimizer as inputs to train_step\n",
    "'''\n",
    "#@tf.function #this creates the tf graph\n",
    "def model(inputs, enc_mods, dec_mods, is_train=False):\n",
    "    \n",
    "    '''\n",
    "    Multiscale autoencoder, taken from Hasegawa 2020. The contribution of the CNNs at different\n",
    "    scales are simply summed.\n",
    "    '''\n",
    "        \n",
    "    # sum of the contributions of the different CNNs\n",
    "    encoded = 0\n",
    "    for enc_mod in enc_mods:\n",
    "        encoded += enc_mod(inputs, training=is_train)\n",
    "            \n",
    "    decoded = 0\n",
    "    for dec_mod in dec_mods:\n",
    "        decoded += dec_mod(encoded, training=is_train)\n",
    "        \n",
    "    return encoded, decoded\n",
    "\n",
    "\n",
    "#@tf.function #this creates the tf graph\n",
    "def train_step(inputs, enc_mods, dec_mods, Loss_Mse, optimizer, train=True):\n",
    "    \n",
    "    \"\"\"\n",
    "    Trains the model by minimizing the loss between input and output\n",
    "    \"\"\"\n",
    "    \n",
    "    # autoencoded field\n",
    "    decoded  = model(inputs, enc_mods, dec_mods, is_train=train)[-1]\n",
    "\n",
    "    # loss with respect to the data\n",
    "    loss = Loss_Mse(inputs, decoded)\n",
    "    \n",
    "    # compute and apply gradients inside tf.function environment for computational efficiency\n",
    "    if train:\n",
    "        # create a variable with all the weights to perform gradient descent on\n",
    "        # appending lists is done by plus sign\n",
    "        varss = [] #+ Dense.trainable_weights\n",
    "        for enc_mod in enc_mods:\n",
    "            varss  += enc_mod.trainable_weights\n",
    "        for dec_mod in dec_mods:\n",
    "            varss +=  dec_mod.trainable_weights\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            decoded  = model(inputs, enc_mods, dec_mods, is_train=train)[-1]\n",
    "            loss = Loss_Mse(inputs, decoded)\n",
    "        grads = tape.gradient(loss, varss)\n",
    "        optimizer.apply_gradients(zip(grads, varss))\n",
    "    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74a2c27782f3dc56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T20:41:49.190554600Z",
     "start_time": "2025-01-14T20:41:49.150752600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def periodic_padding(image, padding=1, asym=False):\n",
    "    '''\n",
    "    Create a periodic padding (same of np.pad('wrap')) around the image, \n",
    "    to mimic periodic boundary conditions.\n",
    "    When asym=True on the right and lower edges an additional column/row is added\n",
    "    '''\n",
    "        \n",
    "    if asym:\n",
    "        lower_pad = image[:,:padding+1,:]\n",
    "    else:\n",
    "        lower_pad = image[:,:padding,:]\n",
    "    \n",
    "    if padding != 0:\n",
    "        upper_pad     = image[:,-padding:,:]\n",
    "        partial_image = tf.concat([upper_pad, image, lower_pad], axis=1)\n",
    "    else:\n",
    "        partial_image = tf.concat([image, lower_pad], axis=1)\n",
    "        \n",
    "    if asym:\n",
    "        right_pad = partial_image[:,:,:padding+1] \n",
    "    else:\n",
    "        right_pad = partial_image[:,:,:padding]\n",
    "    \n",
    "    if padding != 0:\n",
    "        left_pad = partial_image[:,:,-padding:]\n",
    "        padded_image = tf.concat([left_pad, partial_image, right_pad], axis=2)\n",
    "    else:\n",
    "        padded_image = tf.concat([partial_image, right_pad], axis=2)\n",
    "\n",
    "    return padded_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2f5299236c25069",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T20:41:50.186009500Z",
     "start_time": "2025-01-14T20:41:50.129402900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PerPad2D(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Periodic Padding layer\n",
    "    \"\"\"\n",
    "    def __init__(self, padding=1, asym=False, **kwargs):\n",
    "        self.padding = padding\n",
    "        self.asym    = asym\n",
    "        super(PerPad2D, self).__init__(**kwargs)\n",
    "        \n",
    "    def get_config(self): #needed to be able to save and load the model with this layer\n",
    "        config = super(PerPad2D, self).get_config()\n",
    "        config.update({\n",
    "            'padding': self.padding,\n",
    "            'asym': self.asym,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def call(self, x):\n",
    "        return periodic_padding(x, self.padding, self.asym)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e08e201bf0dd289",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Create the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "401b86b6b0639604",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T20:41:54.031884800Z",
     "start_time": "2025-01-14T20:41:53.997673700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "TODO: hyperparameter tuning\n",
    "'''\n",
    "\n",
    "## define the model\n",
    "# we do not have pooling and upsampling, instead we use stride=2\n",
    "N_lat         = 5                          # change N_lat hyperparameter\n",
    "last_conv_dep = 1                          # output depth of last conv layer, if we want to include dissipation rate and vorticity, increase this number\n",
    "n_fil         = [6,12,24,last_conv_dep]          #number of filters encoder\n",
    "n_dec         = [24,12,6,3]                #number of filters decoder\n",
    "N_parallel    = 3                          #number of parallel CNNs for multiscale\n",
    "ker_size      = [(3,3), (5,5), (7,7)]      #kernel sizes\n",
    "N_layers      = 4                          #number of layers in every CNN\n",
    "act           = 'tanh'                     #activation function\n",
    "\n",
    "pad_enc       = 'valid'         #no padding in the conv layer\n",
    "pad_dec       = 'valid'\n",
    "p_size        = [0,1,2]         #stride = 2 periodic padding size          \n",
    "p_fin         = [1,2,3]         #stride = 1 periodic padding size\n",
    "p_dec         = 1               #padding in the first decoder layer\n",
    "p_crop        = U.shape[1]      #crop size of the output equal to input size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9b1599575fd10b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T20:41:55.151659900Z",
     "start_time": "2025-01-14T20:41:54.898354600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#initialize the encoders and decoders with different kernel sizes    \n",
    "enc_mods      = [None]*(N_parallel)\n",
    "dec_mods      = [None]*(N_parallel)    \n",
    "for i in range(N_parallel):\n",
    "    enc_mods[i] = tf.keras.Sequential(name='Enc_' + str(i))\n",
    "    dec_mods[i] = tf.keras.Sequential(name='Dec_' + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90d6a2f58552fa24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T20:41:56.017689400Z",
     "start_time": "2025-01-14T20:41:55.751696400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#generate encoder layers    \n",
    "for j in range(N_parallel):\n",
    "    for i in range(N_layers):      \n",
    "\n",
    "        #stride=2 padding and conv\n",
    "        enc_mods[j].add(PerPad2D(padding=p_size[j], asym=True,\n",
    "                                          name='Enc_' + str(j)+'_PerPad_'+str(i)))\n",
    "        enc_mods[j].add(tf.keras.layers.Conv2D(filters = n_fil[i], kernel_size=ker_size[j],\n",
    "                                      activation=act, padding=pad_enc, strides=2,\n",
    "                        name='Enc_' + str(j)+'_ConvLayer_'+str(i)))\n",
    "\n",
    "        #stride=1 padding and conv\n",
    "        if i<N_layers-1:\n",
    "            enc_mods[j].add(PerPad2D(padding=p_fin[j], asym=False,\n",
    "                                                      name='Enc_'+str(j)+'_Add_PerPad1_'+str(i)))\n",
    "            enc_mods[j].add(tf.keras.layers.Conv2D(filters=n_fil[i],\n",
    "                                                    kernel_size=ker_size[j], \n",
    "                                                activation=act,padding=pad_dec,strides=1,\n",
    "                                                    name='Enc_'+str(j)+'_Add_Layer1_'+str(i)))  \n",
    "    # Add fully connected layer\n",
    "    enc_mods[j].add(tf.keras.layers.Flatten(name='Enc_' + str(j) + '_Flatten'))\n",
    "    enc_mods[j].add(tf.keras.layers.Dense(N_lat, activation='linear', name='Enc_' + str(j) + '_Dense'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1beb0a62c6bf7543",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T20:41:57.142614100Z",
     "start_time": "2025-01-14T20:41:56.725875600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape of the last convolutional layer: (3, 3, 1)\n",
      "SIze of last convolutional output:  9\n",
      "Size of the latent space: 5\n"
     ]
    }
   ],
   "source": [
    "#explicitly obtain the size of the latent space\n",
    "output = U_train[0]\n",
    "for i, layer in enumerate(enc_mods[-1].layers):\n",
    "    output = layer(output)  # Forward pass through the current layer\n",
    "    if i == (N_layers - 1) * 4 + 1:  # Stop after the 4th layer (index 3)\n",
    "        conv_out_shape = output.shape[1:]\n",
    "        conv_out_size = np.prod(conv_out_shape)\n",
    "        print(\"Output shape of the last convolutional layer:\", conv_out_shape)\n",
    "        print(\"SIze of last convolutional output: \", conv_out_size)\n",
    "    elif i == (N_layers - 1) * 4 + 2 + 1:\n",
    "         print(\"Size of the latent space:\", output.shape[-1])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c854b595c114f84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T20:42:02.402721Z",
     "start_time": "2025-01-14T20:42:02.331698600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_#generate decoder layers            \n",
    "for j in range(N_parallel):\n",
    "    \n",
    "    # Add fully connected layer first to map latent space to the appropriate dimensions\n",
    "    dec_mods[j].add(tf.keras.layers.Dense(conv_out_size, activation='linear', name='Dec_' + str(j) + '_Dense'))\n",
    "    dec_mods[j].add(tf.keras.layers.Reshape(conv_out_shape, name='Dec_' + str(j) + '_Reshape'))\n",
    "\n",
    "\n",
    "    for i in range(N_layers):\n",
    "\n",
    "        #initial padding of latent space\n",
    "        if i==0: \n",
    "            dec_mods[j].add(PerPad2D(padding=p_dec, asym=False,\n",
    "                                          name='Dec_' + str(j)+'_PerPad_'+str(i))) \n",
    "        \n",
    "        #Transpose convolution with stride = 2 \n",
    "        dec_mods[j].add(tf.keras.layers.Conv2DTranspose(filters = n_dec[i],\n",
    "                                       output_padding=None,kernel_size=ker_size[j],\n",
    "                                      activation=act, padding=pad_dec, strides=2,\n",
    "                            name='Dec_' + str(j)+'_ConvLayer_'+str(i)))\n",
    "        \n",
    "        #Convolution with stride=1\n",
    "        if  i<N_layers-1:       \n",
    "            dec_mods[j].add(tf.keras.layers.Conv2D(filters=n_dec[i],\n",
    "                                        kernel_size=ker_size[j], \n",
    "                                       activation=act,padding=pad_dec,strides=1,\n",
    "                                      name='Dec_' + str(j)+'_ConvLayer1_'+str(i)))\n",
    "\n",
    "    #crop and final linear convolution with stride=1\n",
    "    dec_mods[j].add(tf.keras.layers.CenterCrop(p_crop + 2*p_fin[j],\n",
    "                                                   p_crop+ 2*p_fin[j],\n",
    "                            name='Dec_' + str(j)+'_Crop_'+str(i)))\n",
    "    dec_mods[j].add(tf.keras.layers.Conv2D(filters=U.shape[3],\n",
    "                                            kernel_size=ker_size[j], \n",
    "                                            activation='linear',padding=pad_dec,strides=1,\n",
    "                                              name='Dec_' + str(j)+'_Final_Layer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14958102f40640fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T20:42:07.836248800Z",
     "start_time": "2025-01-14T20:42:03.544303700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latent space size: 5\n",
      "physical space size: (4608,)\n",
      "\n",
      "Model: \"Enc_0\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Enc_0_PerPad_0 (PerPad2D)   (40, 49, 49, 2)           0         \n",
      "                                                                 \n",
      " Enc_0_ConvLayer_0 (Conv2D)  (40, 24, 24, 6)           114       \n",
      "                                                                 \n",
      " Enc_0_Add_PerPad1_0 (PerPad  (40, 26, 26, 6)          0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " Enc_0_Add_Layer1_0 (Conv2D)  (40, 24, 24, 6)          330       \n",
      "                                                                 \n",
      " Enc_0_PerPad_1 (PerPad2D)   (40, 25, 25, 6)           0         \n",
      "                                                                 \n",
      " Enc_0_ConvLayer_1 (Conv2D)  (40, 12, 12, 12)          660       \n",
      "                                                                 \n",
      " Enc_0_Add_PerPad1_1 (PerPad  (40, 14, 14, 12)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " Enc_0_Add_Layer1_1 (Conv2D)  (40, 12, 12, 12)         1308      \n",
      "                                                                 \n",
      " Enc_0_PerPad_2 (PerPad2D)   (40, 13, 13, 12)          0         \n",
      "                                                                 \n",
      " Enc_0_ConvLayer_2 (Conv2D)  (40, 6, 6, 24)            2616      \n",
      "                                                                 \n",
      " Enc_0_Add_PerPad1_2 (PerPad  (40, 8, 8, 24)           0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " Enc_0_Add_Layer1_2 (Conv2D)  (40, 6, 6, 24)           5208      \n",
      "                                                                 \n",
      " Enc_0_PerPad_3 (PerPad2D)   (40, 7, 7, 24)            0         \n",
      "                                                                 \n",
      " Enc_0_ConvLayer_3 (Conv2D)  (40, 3, 3, 1)             217       \n",
      "                                                                 \n",
      " Enc_0_Flatten (Flatten)     (40, 9)                   0         \n",
      "                                                                 \n",
      " Enc_0_Dense (Dense)         (40, 5)                   50        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,503\n",
      "Trainable params: 10,503\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"Enc_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Enc_1_PerPad_0 (PerPad2D)   (40, 51, 51, 2)           0         \n",
      "                                                                 \n",
      " Enc_1_ConvLayer_0 (Conv2D)  (40, 24, 24, 6)           306       \n",
      "                                                                 \n",
      " Enc_1_Add_PerPad1_0 (PerPad  (40, 28, 28, 6)          0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " Enc_1_Add_Layer1_0 (Conv2D)  (40, 24, 24, 6)          906       \n",
      "                                                                 \n",
      " Enc_1_PerPad_1 (PerPad2D)   (40, 27, 27, 6)           0         \n",
      "                                                                 \n",
      " Enc_1_ConvLayer_1 (Conv2D)  (40, 12, 12, 12)          1812      \n",
      "                                                                 \n",
      " Enc_1_Add_PerPad1_1 (PerPad  (40, 16, 16, 12)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " Enc_1_Add_Layer1_1 (Conv2D)  (40, 12, 12, 12)         3612      \n",
      "                                                                 \n",
      " Enc_1_PerPad_2 (PerPad2D)   (40, 15, 15, 12)          0         \n",
      "                                                                 \n",
      " Enc_1_ConvLayer_2 (Conv2D)  (40, 6, 6, 24)            7224      \n",
      "                                                                 \n",
      " Enc_1_Add_PerPad1_2 (PerPad  (40, 10, 10, 24)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " Enc_1_Add_Layer1_2 (Conv2D)  (40, 6, 6, 24)           14424     \n",
      "                                                                 \n",
      " Enc_1_PerPad_3 (PerPad2D)   (40, 9, 9, 24)            0         \n",
      "                                                                 \n",
      " Enc_1_ConvLayer_3 (Conv2D)  (40, 3, 3, 1)             601       \n",
      "                                                                 \n",
      " Enc_1_Flatten (Flatten)     (40, 9)                   0         \n",
      "                                                                 \n",
      " Enc_1_Dense (Dense)         (40, 5)                   50        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28,935\n",
      "Trainable params: 28,935\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"Enc_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Enc_2_PerPad_0 (PerPad2D)   (40, 53, 53, 2)           0         \n",
      "                                                                 \n",
      " Enc_2_ConvLayer_0 (Conv2D)  (40, 24, 24, 6)           594       \n",
      "                                                                 \n",
      " Enc_2_Add_PerPad1_0 (PerPad  (40, 30, 30, 6)          0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " Enc_2_Add_Layer1_0 (Conv2D)  (40, 24, 24, 6)          1770      \n",
      "                                                                 \n",
      " Enc_2_PerPad_1 (PerPad2D)   (40, 29, 29, 6)           0         \n",
      "                                                                 \n",
      " Enc_2_ConvLayer_1 (Conv2D)  (40, 12, 12, 12)          3540      \n",
      "                                                                 \n",
      " Enc_2_Add_PerPad1_1 (PerPad  (40, 18, 18, 12)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " Enc_2_Add_Layer1_1 (Conv2D)  (40, 12, 12, 12)         7068      \n",
      "                                                                 \n",
      " Enc_2_PerPad_2 (PerPad2D)   (40, 17, 17, 12)          0         \n",
      "                                                                 \n",
      " Enc_2_ConvLayer_2 (Conv2D)  (40, 6, 6, 24)            14136     \n",
      "                                                                 \n",
      " Enc_2_Add_PerPad1_2 (PerPad  (40, 12, 12, 24)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " Enc_2_Add_Layer1_2 (Conv2D)  (40, 6, 6, 24)           28248     \n",
      "                                                                 \n",
      " Enc_2_PerPad_3 (PerPad2D)   (40, 11, 11, 24)          0         \n",
      "                                                                 \n",
      " Enc_2_ConvLayer_3 (Conv2D)  (40, 3, 3, 1)             1177      \n",
      "                                                                 \n",
      " Enc_2_Flatten (Flatten)     (40, 9)                   0         \n",
      "                                                                 \n",
      " Enc_2_Dense (Dense)         (40, 5)                   50        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 56,583\n",
      "Trainable params: 56,583\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"Dec_0\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Dec_0_Dense (Dense)         (40, 9)                   54        \n",
      "                                                                 \n",
      " Dec_0_Reshape (Reshape)     (40, 3, 3, 1)             0         \n",
      "                                                                 \n",
      " Dec_0_PerPad_0 (PerPad2D)   (40, 5, 5, 1)             0         \n",
      "                                                                 \n",
      " Dec_0_ConvLayer_0 (Conv2DTr  (40, 11, 11, 24)         240       \n",
      " anspose)                                                        \n",
      "                                                                 \n",
      " Dec_0_ConvLayer1_0 (Conv2D)  (40, 9, 9, 24)           5208      \n",
      "                                                                 \n",
      " Dec_0_ConvLayer_1 (Conv2DTr  (40, 19, 19, 12)         2604      \n",
      " anspose)                                                        \n",
      "                                                                 \n",
      " Dec_0_ConvLayer1_1 (Conv2D)  (40, 17, 17, 12)         1308      \n",
      "                                                                 \n",
      " Dec_0_ConvLayer_2 (Conv2DTr  (40, 35, 35, 6)          654       \n",
      " anspose)                                                        \n",
      "                                                                 \n",
      " Dec_0_ConvLayer1_2 (Conv2D)  (40, 33, 33, 6)          330       \n",
      "                                                                 \n",
      " Dec_0_ConvLayer_3 (Conv2DTr  (40, 67, 67, 3)          165       \n",
      " anspose)                                                        \n",
      "                                                                 \n",
      " Dec_0_Crop_3 (CenterCrop)   (40, 50, 50, 3)           0         \n",
      "                                                                 \n",
      " Dec_0_Final_Layer (Conv2D)  (40, 48, 48, 2)           56        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,619\n",
      "Trainable params: 10,619\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"Dec_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Dec_1_Dense (Dense)         (40, 9)                   54        \n",
      "                                                                 \n",
      " Dec_1_Reshape (Reshape)     (40, 3, 3, 1)             0         \n",
      "                                                                 \n",
      " Dec_1_PerPad_0 (PerPad2D)   (40, 5, 5, 1)             0         \n",
      "                                                                 \n",
      " Dec_1_ConvLayer_0 (Conv2DTr  (40, 13, 13, 24)         624       \n",
      " anspose)                                                        \n",
      "                                                                 \n",
      " Dec_1_ConvLayer1_0 (Conv2D)  (40, 9, 9, 24)           14424     \n",
      "                                                                 \n",
      " Dec_1_ConvLayer_1 (Conv2DTr  (40, 21, 21, 12)         7212      \n",
      " anspose)                                                        \n",
      "                                                                 \n",
      " Dec_1_ConvLayer1_1 (Conv2D)  (40, 17, 17, 12)         3612      \n",
      "                                                                 \n",
      " Dec_1_ConvLayer_2 (Conv2DTr  (40, 37, 37, 6)          1806      \n",
      " anspose)                                                        \n",
      "                                                                 \n",
      " Dec_1_ConvLayer1_2 (Conv2D)  (40, 33, 33, 6)          906       \n",
      "                                                                 \n",
      " Dec_1_ConvLayer_3 (Conv2DTr  (40, 69, 69, 3)          453       \n",
      " anspose)                                                        \n",
      "                                                                 \n",
      " Dec_1_Crop_3 (CenterCrop)   (40, 52, 52, 3)           0         \n",
      "                                                                 \n",
      " Dec_1_Final_Layer (Conv2D)  (40, 48, 48, 2)           152       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 29,243\n",
      "Trainable params: 29,243\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"Dec_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Dec_2_Dense (Dense)         (40, 9)                   54        \n",
      "                                                                 \n",
      " Dec_2_Reshape (Reshape)     (40, 3, 3, 1)             0         \n",
      "                                                                 \n",
      " Dec_2_PerPad_0 (PerPad2D)   (40, 5, 5, 1)             0         \n",
      "                                                                 \n",
      " Dec_2_ConvLayer_0 (Conv2DTr  (40, 15, 15, 24)         1200      \n",
      " anspose)                                                        \n",
      "                                                                 \n",
      " Dec_2_ConvLayer1_0 (Conv2D)  (40, 9, 9, 24)           28248     \n",
      "                                                                 \n",
      " Dec_2_ConvLayer_1 (Conv2DTr  (40, 23, 23, 12)         14124     \n",
      " anspose)                                                        \n",
      "                                                                 \n",
      " Dec_2_ConvLayer1_1 (Conv2D)  (40, 17, 17, 12)         7068      \n",
      "                                                                 \n",
      " Dec_2_ConvLayer_2 (Conv2DTr  (40, 39, 39, 6)          3534      \n",
      " anspose)                                                        \n",
      "                                                                 \n",
      " Dec_2_ConvLayer1_2 (Conv2D)  (40, 33, 33, 6)          1770      \n",
      "                                                                 \n",
      " Dec_2_ConvLayer_3 (Conv2DTr  (40, 71, 71, 3)          885       \n",
      " anspose)                                                        \n",
      "                                                                 \n",
      " Dec_2_Crop_3 (CenterCrop)   (40, 54, 54, 3)           0         \n",
      "                                                                 \n",
      " Dec_2_Final_Layer (Conv2D)  (40, 48, 48, 2)           296       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 57,179\n",
      "Trainable params: 57,179\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# run the model once to print summary\n",
    "enc0, dec0 = model(U_train[0], enc_mods, dec_mods)\n",
    "print('latent space size:', N_lat)\n",
    "print('physical space size:', U[0].flatten().shape)\n",
    "print('')\n",
    "for j in range(3):\n",
    "    enc_mods[j].summary()\n",
    "for j in range(3):\n",
    "    dec_mods[j].summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0da56cc49b2da96",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e97c3210b82d433",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T20:54:23.216933700Z",
     "start_time": "2025-01-14T20:54:22.988806800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "rng = np.random.default_rng() # random generator for later shuffling\n",
    "\n",
    "#define loss, optimizer and initial learning rate   \n",
    "Loss_Mse    = tf.keras.losses.MeanSquaredError()\n",
    "optimizer   = tf.keras.optimizers.Adam(amsgrad=True) #amsgrad True for better convergence\n",
    "l_rate      = 0.002\n",
    "optimizer.learning_rate = l_rate\n",
    "\n",
    "# quantities to check and store the training and validation loss and the training goes on\n",
    "old_loss      = np.zeros(n_epochs) #needed to evaluate training loss convergence to update l_rate\n",
    "tloss_plot    = np.zeros(n_epochs)      # training loss\n",
    "vloss_plot    = np.zeros(n_epochs)      # validation loss\n",
    "N_check       = 1                      # each N_check epochs we check convergence and validation loss\n",
    "patience      = 20                       # if the val_loss has not gone down in the last patience epochs, early stop\n",
    "last_save     = patience               # last epoch where the model was saved\n",
    "\n",
    "# Hyperparameters for changing learning rate\n",
    "N_lr = 5\n",
    "lrate_update = True\n",
    "lrate_mult = 0.75\n",
    "\n",
    "N_plot = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4517fe92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T20:54:23.643706800Z",
     "start_time": "2025-01-14T20:54:23.607444700Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_model(model_path, enc_mods, dec_mods, ker_size, N_lat):\n",
    "    print('Saving Model..')\n",
    "    Path(model_path).mkdir(parents=True, exist_ok=True) #creates directory even when it exists\n",
    "    for i in range(N_parallel):\n",
    "        enc_mods[i].save(model_path + '/enc_mod'+str(ker_size[i])+'_'+str(N_lat)+'.h5')\n",
    "        dec_mods[i].save(model_path + '/dec_mod'+str(ker_size[i])+'_'+str(N_lat)+'.h5')\n",
    "        enc_mods[i].save_weights(model_path + '/enc_mod'+str(ker_size[i])+'_'+str(N_lat)+'_weights.h5')\n",
    "        dec_mods[i].save_weights(model_path + '/dec_mod'+str(ker_size[i])+'_'+str(N_lat)+'_weights.h5')\n",
    "\n",
    "def save_optimizer_params(path, optimizer):\n",
    "    min_weights = optimizer.get_weights()\n",
    "    hf = h5py.File(path + '/opt_weights.h5','w')\n",
    "    for i in range(len(min_weights)):\n",
    "        hf.create_dataset('weights_'+str(i),data=min_weights[i])\n",
    "    hf.create_dataset('length', data=i)\n",
    "    hf.create_dataset('l_rate', data=optimizer.learning_rate)  \n",
    "    hf.close()\n",
    "\n",
    "def our_load_model(model_path, N_parallel, ker_size, N_lat, enc_mods, dec_mods):\n",
    "    print('LOADING MINIMUM')\n",
    "\n",
    "    for i in range(N_parallel):\n",
    "        enc_mods[i].load_weights(model_path + '/enc_mod'+str(ker_size[i])+'_'+str(N_lat)+'_weights.h5')\n",
    "        dec_mods[i].load_weights(model_path + '/dec_mod'+str(ker_size[i])+'_'+str(N_lat)+'_weights.h5')\n",
    "        \n",
    "    return enc_mods, dec_mods\n",
    "\n",
    "def plot_training_curve(vloss_plot, tloss_plot, N_check, epoch): \n",
    "    plt.rcParams[\"figure.figsize\"] = (15,4)\n",
    "    plt.rcParams[\"font.size\"]  = 20\n",
    "    plt.title('MSE convergence')\n",
    "    plt.yscale('log')\n",
    "    plt.grid(True, axis=\"both\", which='both', ls=\"-\", alpha=0.3)\n",
    "    plt.plot(tloss_plot[np.nonzero(tloss_plot)], 'y', label='Train loss')\n",
    "    plt.plot(np.arange(np.nonzero(vloss_plot)[0].shape[0])*N_check,\n",
    "                vloss_plot[np.nonzero(vloss_plot)], label='Val loss')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.legend()    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'MSE{epoch}.pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a554393c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-01-14T20:57:02.212116Z"
    },
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  20%|██        | 1/5 [00:10<00:42, 10.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 ; Train_Loss 0.4031 ; Val_Loss 0.1849 ; Ratio 0.4588\n",
      "Time per epoch 10.61 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  40%|████      | 2/5 [00:20<00:30, 10.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 ; Train_Loss 0.1550 ; Val_Loss 0.1311 ; Ratio 0.8463\n",
      "Time per epoch 9.71 seconds\n",
      "\n",
      "Saving Model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  60%|██████    | 3/5 [00:30<00:20, 10.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 ; Train_Loss 0.1231 ; Val_Loss 0.1085 ; Ratio 0.8821\n",
      "Time per epoch 10.50 seconds\n",
      "\n",
      "Saving Model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  80%|████████  | 4/5 [00:41<00:10, 10.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 ; Train_Loss 0.1065 ; Val_Loss 0.1022 ; Ratio 0.9593\n",
      "Time per epoch 10.52 seconds\n",
      "\n",
      "Saving Model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 5/5 [00:52<00:00, 10.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 ; Train_Loss 0.0987 ; Val_Loss 0.0889 ; Ratio 0.9004\n",
      "Time per epoch 10.84 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange, tqdm_notebook\n",
    "\n",
    "t = time.time()  # Initialize time for printing time per epoch \n",
    "model_path = './data/48_RE40_'+str(N_lat) # to save model\n",
    "\n",
    "for epoch in trange(n_epochs, desc='Epochs'):\n",
    "    \n",
    "    # Incorporate early stopping\n",
    "    if epoch - last_save > patience:\n",
    "        print('Early stopping')\n",
    "        break\n",
    "                \n",
    "    # Perform gradient descent for all the batches every epoch\n",
    "    loss_0 = 0\n",
    "    rng.shuffle(U_train, axis=0) # shuffle batches\n",
    "    for j in range(train_batches):\n",
    "        loss    = train_step(U_train[j], enc_mods, dec_mods, Loss_Mse, optimizer)\n",
    "        loss_0 += loss\n",
    "    \n",
    "    # save train loss\n",
    "    tloss_plot[epoch]  = loss_0.numpy()/train_batches\n",
    "    \n",
    "    # every N epochs checks the convergence of the training loss and val loss\n",
    "    if (epoch%N_check==0):\n",
    "        \n",
    "        # Compute Validation Loss\n",
    "        loss_val        = 0\n",
    "        for j in range(val_batches):\n",
    "            loss        = train_step(U_val[j], enc_mods, dec_mods, Loss_Mse, optimizer, train=False)\n",
    "            loss_val   += loss\n",
    "        \n",
    "        # Save validation loss\n",
    "        vloss_plot[epoch]  = loss_val.numpy()/val_batches \n",
    "\n",
    "        # Decreases the learning rate if the training loss is not going down with respect to \n",
    "        # N_lr epochs before\n",
    "        if epoch > N_lr and lrate_update:\n",
    "            #check if the training loss is smaller than the average training loss N_lr epochs ago\n",
    "            tt_loss   = np.mean(tloss_plot[epoch-N_lr:epoch])\n",
    "            if tt_loss > old_loss[epoch-N_lr]:\n",
    "                #if it is larger, load optimal val loss weights and decrease learning rate\n",
    "                enc_mods, dec_mods = our_load_model(model_path, N_parallel, ker_size, N_lat, enc_mods, dec_mods)\n",
    "\n",
    "                optimizer.learning_rate = optimizer.learning_rate*lrate_mult\n",
    "                min_weights = optimizer.get_weights() # RV - just added this line\n",
    "                optimizer.set_weights(min_weights)\n",
    "                print('LEARNING RATE CHANGE', optimizer.learning_rate.numpy())\n",
    "                old_loss[epoch-N_lr:epoch] = 1e6 #so that l_rate is not changed for N_lr steps\n",
    "        \n",
    "        #store current loss\n",
    "        old_loss[epoch] = tloss_plot[epoch].copy()\n",
    "        \n",
    "        # save best model (the one with minimum validation loss)\n",
    "        if epoch > 1 and vloss_plot[epoch] < \\\n",
    "                         (vloss_plot[:epoch-1][np.nonzero(vloss_plot[:epoch-1])]).min():\n",
    "        \n",
    "            #saving the model weights\n",
    "            save_model(model_path, enc_mods, dec_mods, ker_size, N_lat)\n",
    "\n",
    "            #saving optimizer parameters\n",
    "            save_optimizer_params(model_path, optimizer)\n",
    "            \n",
    "            last_save = epoch #store the last time the val loss has decreased for early stop\n",
    "\n",
    "        # Print loss values and training time (per epoch)\n",
    "        print('Epoch', epoch, '; Train_Loss', f\"{tloss_plot[epoch]:.4f}\", \n",
    "              '; Val_Loss', f\"{vloss_plot[epoch]:.4f}\",  '; Ratio', f\"{(vloss_plot[epoch])/(tloss_plot[epoch]):.4f}\")\n",
    "        print(f'Time per epoch {(time.time()-t):.2f} seconds')\n",
    "        print('')\n",
    "        t = time.time()  # Reset time after each epoch\n",
    "\n",
    "    if (epoch%N_plot==0) and epoch != 0: # Plot every N_plot epochs\n",
    "        plot_training_curve(vloss_plot, tloss_plot, N_check, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a68c1040",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T15:00:42.122210600Z",
     "start_time": "2025-01-09T15:00:41.449890900Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Set figure size\n",
    "# plt.figure(figsize=(12, 4))  # Width = 6 inches, Height = 4 inches\n",
    "\n",
    "# plt.title('MSE convergence')\n",
    "# plt.yscale('log')\n",
    "# plt.grid(True, axis=\"both\", which='both', ls=\"-\", alpha=0.3)\n",
    "# plt.plot(tloss_plot[np.nonzero(tloss_plot)], 'y', label='Train loss')\n",
    "# plt.plot(np.arange(np.nonzero(vloss_plot)[0].shape[0]) * N_check, vloss_plot[np.nonzero(vloss_plot)], label='Val loss')\n",
    "# plt.xlabel('epochs')\n",
    "# plt.legend()    \n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# print(\"Plotting done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2725d733",
   "metadata": {},
   "source": [
    "# Compute the error over the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4153af7dd84b15cd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute the NRMSE for the test data by comparing the encoded data and the ground truth data\n",
    "def compute_nrmse(U_test, U_pred):\n",
    "    \"\"\"\n",
    "    Compute the Normalized Root-Mean-Square Error (NRMSE) for batched flowfields.\n",
    "    \n",
    "    Args:\n",
    "        U_test: Tensor of shape [n_batches, batch_size, 48, 48, 2], the ground truth flowfield.\n",
    "        U_pred: Tensor of shape [n_batches, batch_size, 48, 48, 2], the predicted (reconstructed) flowfield.\n",
    "\n",
    "    Returns:\n",
    "        nrmse: A scalar Tensor, the computed NRMSE across all batches.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure both tensors have the same dtype\n",
    "    U_test = tf.cast(U_test, dtype=tf.float32)\n",
    "    U_pred = tf.cast(U_pred, dtype=tf.float32)\n",
    "\n",
    "    # Compute the mean squared error (MSE) for each batch\n",
    "    axes = [2, 3, 4] # reduces the 48, 48, 2 dimensions\n",
    "\n",
    "    mse = tf.reduce_mean(tf.square(U_pred - U_test), axis=axes)  # Shape: [batch_size]\n",
    "\n",
    "    # Compute the variance of the ground truth (σ²) for normalization\n",
    "    variance = tf.reduce_mean(tf.square(U_test - tf.reduce_mean(U_test, axis=axes, keepdims=True)), axis=axes)\n",
    "\n",
    "    # Compute the NRMSE for each batch\n",
    "    nrmse_per_batch = tf.sqrt(mse / variance)  # Shape: [batch_size]\n",
    "\n",
    "    # Average NRMSE across the batch\n",
    "    nrmse = tf.reduce_mean(nrmse_per_batch)\n",
    "\n",
    "    return nrmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "891c5c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING MINIMUM\n",
      "NRMSE: 0.32341552\n"
     ]
    }
   ],
   "source": [
    "model_path = './data/48_RE40_'+str(N_lat)\n",
    "enc_mods, dec_mods = our_load_model(model_path, N_parallel, ker_size, N_lat, enc_mods, dec_mods)\n",
    "U_pred = np.zeros((test_batches, b_size, N_x, N_y, 2))\n",
    "for i in range(test_batches):\n",
    "    U_pred[i] = model(U_test[i], enc_mods, dec_mods, is_train=False)[-1]\n",
    "\n",
    "nrmse = compute_nrmse(U_test, U_pred)\n",
    "print(\"NRMSE:\", nrmse.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8cee80fba0cc06",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Visualize error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c6c933e765a4b30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T15:00:44.402317Z",
     "start_time": "2025-01-09T15:00:42.118730600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models: encoder -> ./data/48_RE40_5/enc_mod(3, 3)_5.h5, decoder -> ./data/48_RE40_5/dec_mod(3, 3)_5.h5\n",
      "Loading models: encoder -> ./data/48_RE40_5/enc_mod(5, 5)_5.h5, decoder -> ./data/48_RE40_5/dec_mod(5, 5)_5.h5\n",
      "Loading models: encoder -> ./data/48_RE40_5/enc_mod(7, 7)_5.h5, decoder -> ./data/48_RE40_5/dec_mod(7, 7)_5.h5\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#load model for the test set\n",
    "path = './data/48_RE40_'+str(N_lat)\n",
    "os.makedirs(path, exist_ok=True)\n",
    "\n",
    "enc_mods_test = [None] * N_parallel\n",
    "dec_mods_test = [None] * N_parallel\n",
    "\n",
    "for i in range(N_parallel):\n",
    "    enc_path = f\"{path}/enc_mod{ker_size[i]}_{N_lat}.h5\"\n",
    "    dec_path = f\"{path}/dec_mod{ker_size[i]}_{N_lat}.h5\"\n",
    "    print(f\"Loading models: encoder -> {enc_path}, decoder -> {dec_path}\")\n",
    "\n",
    "    if os.path.exists(enc_path) and os.path.exists(dec_path):\n",
    "        try:\n",
    "            enc_mods_test[i] = tf.keras.models.load_model(enc_path, custom_objects={\"PerPad2D\": PerPad2D})\n",
    "            dec_mods_test[i] = tf.keras.models.load_model(dec_path, custom_objects={\"PerPad2D\": PerPad2D})\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error loading models: {e}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Model files not found: {enc_path}, {dec_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1294525edbb68e86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T15:00:53.025501800Z",
     "start_time": "2025-01-09T15:00:44.436139600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "# grid\n",
    "X = np.linspace(0, 2 * np.pi, N_x) \n",
    "Y = np.linspace(0, 2 * np.pi, N_y) \n",
    "XX = np.meshgrid(X, Y, indexing='ij')\n",
    "\n",
    "# plot n snapshots and their reconstruction in the test set.\n",
    "n_snapshots = 5\n",
    "plt.rcParams[\"figure.figsize\"] = (15, 4 * n_snapshots)\n",
    "plt.rcParams[\"font.size\"] = 14\n",
    "fig, ax = plt.subplots(n_snapshots, 3)\n",
    "\n",
    "# start after validation set\n",
    "start = 0 # b_size * n_batches * downsample + b_size * val_batches * downsample\n",
    "print(start)\n",
    "\n",
    "# Function to wrap text\n",
    "def wrap_text(text, width=25):\n",
    "    return \"\\n\".join(textwrap.wrap(text, width))\n",
    "\n",
    "for i in range(n_snapshots):\n",
    "    # testing data\n",
    "    skips = 10\n",
    "    # u = U_test_unbatched[i*100:i*100+1].copy()\n",
    "    u      = U_test_unbatched[start+10+i*skips:start+11+i*skips].copy() \n",
    "    vmax = u.max()\n",
    "    vmin = u.min()\n",
    "    \n",
    "    # truth\n",
    "    ax_truth = plt.subplot(n_snapshots, 3, i * 3 + 1)\n",
    "    CS0 = ax_truth.contourf(XX[0], XX[1], u[0, :, :, 0],\n",
    "                            levels=10, cmap='coolwarm', vmin=vmin, vmax=vmax)\n",
    "    cbar = plt.colorbar(CS0, ax=ax_truth)\n",
    "    CS = ax_truth.contour(XX[0], XX[1], u[0, :, :, 0],\n",
    "                          levels=10, colors='black', linewidths=.5, linestyles='solid',\n",
    "                          vmin=vmin, vmax=vmax)\n",
    "    title = wrap_text(f'True velocity field at snapshot {i+1}')\n",
    "    ax_truth.set_title(title, pad=20, fontsize=12)\n",
    "\n",
    "    # autoencoded\n",
    "    ax_auto = plt.subplot(n_snapshots, 3, i * 3 + 2)\n",
    "    u_dec = model(u, enc_mods_test, dec_mods_test)[1][0].numpy()\n",
    "    CS = ax_auto.contourf(XX[0], XX[1], u_dec[:, :, 0],\n",
    "                          levels=10, cmap='coolwarm', vmin=vmin, vmax=vmax)\n",
    "    cbar = plt.colorbar(CS, ax=ax_auto)\n",
    "    CS = ax_auto.contour(XX[0], XX[1], u_dec[:, :, 0],\n",
    "                         levels=10, colors='black', linewidths=.5, linestyles='solid',\n",
    "                         vmin=vmin, vmax=vmax)\n",
    "    title = wrap_text(f'Autoencoded velocity field at snapshot {i+1}')\n",
    "    ax_auto.set_title(title, pad=20, fontsize=12)\n",
    "    \n",
    "    # error\n",
    "    ax_err = plt.subplot(n_snapshots, 3, i * 3 + 3)\n",
    "    u_err = np.abs(u_dec - u[0]) / (vmax - vmin)\n",
    "    nmae = u_err[:, :, 0].mean()\n",
    "\n",
    "    CS = ax_err.contourf(XX[0], XX[1], u_err[:, :, 0], levels=10, cmap='coolwarm')\n",
    "    cbar = plt.colorbar(CS, ax=ax_err)\n",
    "    CS = ax_err.contour(XX[0], XX[1], u_err[:, :, 0], levels=10, colors='black', linewidths=.5,\n",
    "                        linestyles='solid')\n",
    "    title = wrap_text(f'Error between velocity fields at snapshot {i+1} with NMAE: {nmae:.4f}')\n",
    "    ax_err.set_title(title, pad=20, fontsize=12)\n",
    "    \n",
    "# Adjust spacing between plots\n",
    "# fig.tight_layout(pad=1.0)  # Increase the padding between subplots\n",
    "# plt.subplots_adjust(wspace=0.3, hspace=0.4)  # Add extra spacing between rows and columns\n",
    "\n",
    "# plt.savefig(path + '/Autoencoder_error.pdf')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fd0737aa30b6ae",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Save Encoded Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "797d64a9be0d48d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T21:09:19.763104500Z",
     "start_time": "2025-01-14T21:09:19.241850700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'N_parallel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m a \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m]\u001b[38;5;241m*\u001b[39m\u001b[43mN_parallel\u001b[49m\n\u001b[0;32m      2\u001b[0m b \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m]\u001b[38;5;241m*\u001b[39mN_parallel\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N_parallel):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'N_parallel' is not defined"
     ]
    }
   ],
   "source": [
    "a = [None]*N_parallel\n",
    "b = [None]*N_parallel\n",
    "\n",
    "for i in range(N_parallel):\n",
    "    a[i] = tf.keras.models.load_model(path + '/enc_mod'+str(ker_size[i])+'_'+str(N_lat)+'.h5', \n",
    "                                            custom_objects={\"PerPad2D\": PerPad2D})\n",
    "    \n",
    "for i in range(N_parallel):\n",
    "    b[i] = tf.keras.models.load_model(path + '/dec_mod'+str(ker_size[i])+'_'+str(N_lat)+'.h5', \n",
    "                                            custom_objects={\"PerPad2D\": PerPad2D})\n",
    "\n",
    "U_enc = np.zeros((n_batches, N_lat))\n",
    "\n",
    "for i in range(train_batches):\n",
    "    U_enc[i] = model(np.expand_dims(U[i], axis=0), a, b)[0].numpy()\n",
    "\n",
    "fln = f'./data/48_Encoded_data_Re{Re}_' + str(N_lat) +'.h5'\n",
    "hf = h5py.File(fln,'w')\n",
    "hf.create_dataset('U_enc',data=U_enc)  \n",
    "hf.close()\n",
    "print(fln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3192609e9d06109",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "precursor_legacy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
