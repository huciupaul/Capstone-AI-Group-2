{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dfee64004020ddf8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "60faf1bcc66cef9b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\" #set cores for numpy\n",
    "import numpy as np\n",
    "import json\n",
    "tf.get_logger().setLevel('ERROR') #no info and warnings are printed \n",
    "tf.config.threading.set_inter_op_parallelism_threads(1) #set cores for TF\n",
    "tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "# tf.config.set_visible_devices([], 'GPU') #runs the code without GPU\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import time\n",
    "from pathlib import Path\n",
    "import matplotlib as mpl\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "#Latex\n",
    "mpl.rc('text', usetex = True)\n",
    "mpl.rc('font', family = 'serif')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Handling"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "927fe4f2d90758e3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data loading"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6bf3de97e6910cb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "'''\n",
    "TODO: Update parameters\n",
    "TODO: remove transient as it was not saved when we generated data\n",
    "'''\n",
    "\n",
    "upsample = 1        # 1 = no upsampling used (if = 10, save every 10 steps.)\n",
    "Re       = 30\n",
    "data_len = 300000\n",
    "transient = 10000\n",
    "Nx = 48\n",
    "Nu = 2\n",
    "# Generate this from Gen_data.ipynb\n",
    "fln = '/data/ar994/Python/data/Kolmogorov/Kolmogorov_0.1_48_30.0_100100_32.h5'\n",
    "hf  = h5py.File(fln,'r')\n",
    "dt  = 0.1\n",
    "U   = np.array(hf.get('U')[transient:transient+data_len:upsample], dtype=np.float32)\n",
    "hf.close()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ceda566dd0d31a7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "N_x     = U.shape[1]\n",
    "N_y     = U.shape[2]\n",
    "\n",
    "print('U-shape:',U.shape, dt)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fad809402ca51b84"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data Split"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "207c6d5934ac3579"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "'''\n",
    "Changes: added skip as input to split_data\n",
    "'''\n",
    "\n",
    "def split_data(U, b_size, n_batches, skip):\n",
    "    \n",
    "    '''\n",
    "    Splits the data in batches. Each batch is created by sampling the signal with interval\n",
    "    equal to n_batches\n",
    "    '''\n",
    "    data   = np.zeros((n_batches, b_size, U.shape[1], U.shape[2], U.shape[3]))    \n",
    "    for j in range(n_batches):\n",
    "        data[j] = U[::skip][j::n_batches].copy()\n",
    "\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a22f0e305d98448"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "b_size      = 50   #batch_size\n",
    "n_batches   = 500  #number of batches\n",
    "val_batches = 50   #int(n_batches*0.2) # validation set size is 0.2 the size of the training set\n",
    "skip        = 10"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b23101c0562d8ab9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# training data\n",
    "print('Train Data%  :',b_size*n_batches*skip/U.shape[0])\n",
    "U_tt        = np.array(U[:b_size*n_batches*skip].copy())            #to be used for random batches\n",
    "U_train     = split_data(U_tt, b_size, n_batches, skip).astype('float32') #to be used for randomly shuffled batches\n",
    "\n",
    "# validation data\n",
    "print('Val   Data%  :',b_size*val_batches*skip/U.shape[0])\n",
    "U_vv        = np.array(U[b_size*n_batches*skip:b_size*n_batches*skip+b_size*val_batches*skip].copy())\n",
    "U_val       = split_data(U_vv, b_size, val_batches, skip).astype('float32') \n",
    "\n",
    "del U_vv, U_tt"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "890776f3731ddf5f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Autoencoder Functions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8879234b2b19161c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "'''\n",
    "Changes: included loss mse and optimizer as inputs to train_step\n",
    "'''\n",
    "@tf.function #this creates the tf graph\n",
    "def model(inputs, enc_mods, dec_mods, is_train=False):\n",
    "    \n",
    "    '''\n",
    "    Multiscale autoencoder, taken from Hasegawa 2020. The contribution of the CNNs at different\n",
    "    scales are simply summed.\n",
    "    '''\n",
    "        \n",
    "    # sum of the contributions of the different CNNs\n",
    "    encoded = 0\n",
    "    for enc_mod in enc_mods:\n",
    "        encoded += enc_mod(inputs, training=is_train)\n",
    "            \n",
    "    decoded = 0\n",
    "    for dec_mod in dec_mods:\n",
    "        decoded += dec_mod(encoded, training=is_train)\n",
    "        \n",
    "    return encoded, decoded\n",
    "\n",
    "\n",
    "@tf.function #this creates the tf graph\n",
    "def train_step(inputs, enc_mods, dec_mods, Loss_Mse, optimizer, train=True):\n",
    "    \n",
    "    \"\"\"\n",
    "    Trains the model by minimizing the loss between input and output\n",
    "    \"\"\"\n",
    "    \n",
    "    # autoencoded field\n",
    "    decoded  = model(inputs, enc_mods, dec_mods, is_train=train)[-1]\n",
    "\n",
    "    # loss with respect to the data\n",
    "    loss     = Loss_Mse(inputs, decoded)\n",
    "    \n",
    "    # compute and apply gradients inside tf.function environment for computational efficiency\n",
    "    if train:\n",
    "        # create a variable with all the weights to perform gradient descent on\n",
    "        # appending lists is done by plus sign\n",
    "        varss    = [] #+ Dense.trainable_weights\n",
    "        for enc_mod in enc_mods:\n",
    "            varss  += enc_mod.trainable_weights\n",
    "        for dec_mod in dec_mods:\n",
    "            varss +=  dec_mod.trainable_weights\n",
    "        \n",
    "        grads   = tf.gradients(loss, varss)\n",
    "        optimizer.apply_gradients(zip(grads, varss))\n",
    "    \n",
    "    return loss\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "24e5f94935abd91d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class PerPad2D(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Periodic Padding layer\n",
    "    \"\"\"\n",
    "    def __init__(self, padding=1, asym=False, **kwargs):\n",
    "        self.padding = padding\n",
    "        self.asym    = asym\n",
    "        super(PerPad2D, self).__init__(**kwargs)\n",
    "        \n",
    "    def get_config(self): #needed to be able to save and load the model with this layer\n",
    "        config = super(PerPad2D, self).get_config()\n",
    "        config.update({\n",
    "            'padding': self.padding,\n",
    "            'asym': self.asym,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def call(self, x):\n",
    "        return periodic_padding(x, self.padding, self.asym)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b2f5299236c25069"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def periodic_padding(image, padding=1, asym=False):\n",
    "    '''\n",
    "    Create a periodic padding (same of np.pad('wrap')) around the image, \n",
    "    to mimic periodic boundary conditions.\n",
    "    When asym=True on the right and lower edges an additional column/row is added\n",
    "    '''\n",
    "        \n",
    "    if asym:\n",
    "        lower_pad = image[:,:padding+1,:]\n",
    "    else:\n",
    "        lower_pad = image[:,:padding,:]\n",
    "    \n",
    "    if padding != 0:\n",
    "        upper_pad     = image[:,-padding:,:]\n",
    "        partial_image = tf.concat([upper_pad, image, lower_pad], axis=1)\n",
    "    else:\n",
    "        partial_image = tf.concat([image, lower_pad], axis=1)\n",
    "        \n",
    "    if asym:\n",
    "        right_pad = partial_image[:,:,:padding+1] \n",
    "    else:\n",
    "        right_pad = partial_image[:,:,:padding]\n",
    "    \n",
    "    if padding != 0:\n",
    "        left_pad = partial_image[:,:,-padding:]\n",
    "        padded_image = tf.concat([left_pad, partial_image, right_pad], axis=2)\n",
    "    else:\n",
    "        padded_image = tf.concat([partial_image, right_pad], axis=2)\n",
    "\n",
    "    return padded_image"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74a2c27782f3dc56"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create the Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e08e201bf0dd289"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "'''\n",
    "TODO: What is lat_dep?\n",
    "TODO: hyperparameter tuning\n",
    "TODO: update N_latent\n",
    "'''\n",
    "\n",
    "# define the model\n",
    "# we do not have pooling and upsampling, instead we use stride=2\n",
    "\n",
    "lat_dep       = 2                          #latent space depth\n",
    "n_fil         = [6,12,24,lat_dep]          #number of filters ecnoder\n",
    "n_dec         = [24,12,6,3]                #number of filters decoder\n",
    "N_parallel    = 3                          #number of parallel CNNs for multiscale\n",
    "ker_size      = [(3,3), (5,5), (7,7)]      #kernel sizes\n",
    "N_layers      = 4                          #number of layers in every CNN\n",
    "act           = 'tanh'                     #activation function\n",
    "\n",
    "pad_enc       = 'valid'         #no padding in the conv layer\n",
    "pad_dec       = 'valid'\n",
    "p_size        = [0,1,2]         #stride = 2 periodic padding size          \n",
    "p_fin         = [1,2,3]         #stride = 1 periodic padding size\n",
    "p_dec         = 1               #padding in the first decoder layer\n",
    "p_crop        = U.shape[1]      #crop size of the output equal to input size\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "401b86b6b0639604"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#initialize the encoders and decoders with different kernel sizes    \n",
    "enc_mods      = [None]*(N_parallel)\n",
    "dec_mods      = [None]*(N_parallel)    \n",
    "for i in range(N_parallel):\n",
    "    enc_mods[i] = tf.keras.Sequential(name='Enc_' + str(i))\n",
    "    dec_mods[i] = tf.keras.Sequential(name='Dec_' + str(i))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9b1599575fd10b5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#generate encoder layers    \n",
    "for j in range(N_parallel):\n",
    "    for i in range(N_layers):      \n",
    "\n",
    "        #stride=2 padding and conv\n",
    "        enc_mods[j].add(PerPad2D(padding=p_size[j], asym=True,\n",
    "                                          name='Enc_' + str(j)+'_PerPad_'+str(i)))\n",
    "        enc_mods[j].add(tf.keras.layers.Conv2D(filters = n_fil[i], kernel_size=ker_size[j],\n",
    "                                      activation=act, padding=pad_enc, strides=2,\n",
    "                        name='Enc_' + str(j)+'_ConvLayer_'+str(i)))\n",
    "\n",
    "        #stride=1 padding and conv\n",
    "        if i<N_layers-1:\n",
    "            enc_mods[j].add(PerPad2D(padding=p_fin[j], asym=False,\n",
    "                                                      name='Enc_'+str(j)+'_Add_PerPad1_'+str(i)))\n",
    "            enc_mods[j].add(tf.keras.layers.Conv2D(filters=n_fil[i],\n",
    "                                                    kernel_size=ker_size[j], \n",
    "                                                activation=act,padding=pad_dec,strides=1,\n",
    "                                                    name='Enc_'+str(j)+'_Add_Layer1_'+str(i)))        "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90d6a2f58552fa24"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#explicitly obtain the size of the latent space\n",
    "N_1      = enc_mods[-1](U_train[0]).shape\n",
    "N_latent = N_1[-3]*N_1[-2]*N_1[-1]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1beb0a62c6bf7543"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#generate decoder layers            \n",
    "for j in range(N_parallel):\n",
    "\n",
    "    for i in range(N_layers):\n",
    "\n",
    "        #initial padding of latent space\n",
    "        if i==0: \n",
    "            dec_mods[j].add(PerPad2D(padding=p_dec, asym=False,\n",
    "                                          name='Dec_' + str(j)+'_PerPad_'+str(i))) \n",
    "        \n",
    "        #Transpose convolution with stride = 2 \n",
    "        dec_mods[j].add(tf.keras.layers.Conv2DTranspose(filters = n_dec[i],\n",
    "                                       output_padding=None,kernel_size=ker_size[j],\n",
    "                                      activation=act, padding=pad_dec, strides=2,\n",
    "                            name='Dec_' + str(j)+'_ConvLayer_'+str(i)))\n",
    "        \n",
    "        #Convolution with stride=1\n",
    "        if  i<N_layers-1:       \n",
    "            dec_mods[j].add(tf.keras.layers.Conv2D(filters=n_dec[i],\n",
    "                                        kernel_size=ker_size[j], \n",
    "                                       activation=act,padding=pad_dec,strides=1,\n",
    "                                      name='Dec_' + str(j)+'_ConvLayer1_'+str(i)))\n",
    "\n",
    "    #crop and final linear convolution with stride=1\n",
    "    dec_mods[j].add(tf.keras.layers.CenterCrop(p_crop + 2*p_fin[j],\n",
    "                                                   p_crop+ 2*p_fin[j],\n",
    "                            name='Dec_' + str(j)+'_Crop_'+str(i)))\n",
    "    dec_mods[j].add(tf.keras.layers.Conv2D(filters=U.shape[3],\n",
    "                                            kernel_size=ker_size[j], \n",
    "                                            activation='linear',padding=pad_dec,strides=1,\n",
    "                                              name='Dec_' + str(j)+'_Final_Layer'))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c854b595c114f84"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# run the model once to print summary\n",
    "enc0, dec0 = model(U_train[0], enc_mods, dec_mods)\n",
    "print('latent   space size:', N_latent)\n",
    "print('physical space size:', U[0].flatten().shape)\n",
    "print('')\n",
    "for j in range(3):\n",
    "    enc_mods[j].summary()\n",
    "for j in range(3):\n",
    "    dec_mods[j].summary()    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14958102f40640fe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train the Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0da56cc49b2da96"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "'''\n",
    "TODO: change path\n",
    "TODO: change get_weights to save_parameters\n",
    "'''\n",
    "\n",
    "rng = np.random.default_rng() #random generator for later shufflinh\n",
    "\n",
    "# plotting and saving\n",
    "plt.rcParams[\"figure.figsize\"] = (15,4)\n",
    "plt.rcParams[\"font.size\"]  = 20\n",
    "path = './data/48_RE30_'+str(N_latent) #to save model\n",
    "\n",
    "\n",
    "#define loss, optimizer and initial learning rate   \n",
    "Loss_Mse    = tf.keras.losses.MeanSquaredError()\n",
    "optimizer   = tf.keras.optimizers.Adam(amsgrad=True) #amsgrad True for better convergence\n",
    "\n",
    "l_rate      = 0.002\n",
    "optimizer.learning_rate = l_rate\n",
    "lrate_update = True     # flag for l_rate updating\n",
    "lrate_mult   = 0.75     # decrease by this factore the l_rate \n",
    "N_lr         = 100      # number of epochs before which the l_rate is not updated\n",
    "\n",
    "\n",
    "# quantities to check and store the training and validation loss and the training goes on\n",
    "old_loss      = np.zeros(n_epochs)      # needed to evaluate training loss convergence to update l_rate\n",
    "tloss_plot    = np.zeros(n_epochs)      # training loss\n",
    "vloss_plot    = np.zeros(n_epochs)      # validation loss\n",
    "old_loss[0]   = 1e6                      # initial value has to be high\n",
    "N_check       = 5                        # each N_check epochs we check convergence and validation loss\n",
    "patience      = 200                      # if the val_loss has not gone down in the last patience epochs, early stop\n",
    "last_save     = patience\n",
    "t             = 1                        # initial (not important value) to monitor the time of the training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e97c3210b82d433"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO: seems to be a problem in line 90\n",
    "\"\"\"\n",
    "\n",
    "n_epochs    = 101   #number of epochs\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    if epoch - last_save > patience: break #early stop\n",
    "                \n",
    "    #Perform gradient descent for all the batches every epoch\n",
    "    loss_0 = 0\n",
    "    rng.shuffle(U_train, axis=0) #shuffle batches\n",
    "    for j in range(n_batches):\n",
    "            loss    = train_step(U_train[j], enc_mods, dec_mods, Loss_Mse, optimizer)\n",
    "            loss_0 += loss\n",
    "    \n",
    "    #save train loss\n",
    "    tloss_plot[epoch]  = loss_0.numpy()/n_batches     \n",
    "    \n",
    "    # every N epochs checks the convergence of the training loss and val loss\n",
    "    if (epoch%N_check==0):\n",
    "        \n",
    "        #Compute Validation Loss\n",
    "        loss_val        = 0\n",
    "        for j in range(val_batches):\n",
    "            loss        = train_step(U_val[j], enc_mods, dec_mods, Loss_Mse, optimizer, train=False)\n",
    "            loss_val   += loss\n",
    "        \n",
    "        #save validation loss\n",
    "        vloss_plot[epoch]  = loss_val.numpy()/val_batches \n",
    "        \n",
    "        # Decreases the learning rate if the training loss is not going down with respect to \n",
    "        # N_lr epochs before\n",
    "        if epoch > N_lr and lrate_update:\n",
    "            #check if the training loss is smaller than the average training loss N_lr epochs ago\n",
    "            tt_loss   = np.mean(tloss_plot[epoch-N_lr:epoch])\n",
    "            if tt_loss > old_loss[epoch-N_lr]:\n",
    "                #if it is larger, load optimal val loss weights and decrease learning rate\n",
    "                print('LOADING MINIMUM')\n",
    "                for i in range(N_parallel):\n",
    "                    enc_mods[i].load_weights(path + '/enc_mod'+str(ker_size[i])+'_'+str(N_latent)+'_weights.h5')\n",
    "                    dec_mods[i].load_weights(path + '/dec_mod'+str(ker_size[i])+'_'+str(N_latent)+'_weights.h5')\n",
    "\n",
    "                optimizer.learning_rate = optimizer.learning_rate*lrate_mult\n",
    "                optimizer.set_weights(min_weights)\n",
    "                print('LEARNING RATE CHANGE', optimizer.learning_rate.numpy(), deviation)\n",
    "                old_loss[epoch-N_lr:epoch] = 1e6 #so that l_rate is not changed for N_lr steps\n",
    "        \n",
    "        #store current loss\n",
    "        old_loss[epoch] = tloss_plot[epoch].copy()\n",
    "        \n",
    "        #save best model (the one with minimum validation loss)\n",
    "        if epoch > 1 and vloss_plot[epoch] < \\\n",
    "                         (vloss_plot[:epoch-1][np.nonzero(vloss_plot[:epoch-1])]).min():\n",
    "        \n",
    "            #saving the model weights\n",
    "            print('Saving Model..')\n",
    "            Path(path).mkdir(parents=True, exist_ok=True) #creates directory even when it exists\n",
    "            for i in range(N_parallel):\n",
    "                enc_mods[i].save(path + '/enc_mod'+str(ker_size[i])+'_'+str(N_latent)+'.h5')\n",
    "                dec_mods[i].save(path + '/dec_mod'+str(ker_size[i])+'_'+str(N_latent)+'.h5')\n",
    "                enc_mods[i].save_weights(path + '/enc_mod'+str(ker_size[i])+'_'+str(N_latent)+'_weights.h5')\n",
    "                dec_mods[i].save_weights(path + '/dec_mod'+str(ker_size[i])+'_'+str(N_latent)+'_weights.h5')\n",
    "            \n",
    "            #saving optimizer parameters\n",
    "            min_weights = optimizer.get_weights()\n",
    "            hf = h5py.File(path + '/opt_weights.h5','w')\n",
    "            for i in range(len(min_weights)):\n",
    "                hf.create_dataset('weights_'+str(i),data=min_weights[i])\n",
    "            hf.create_dataset('length', data=i)\n",
    "            hf.create_dataset('l_rate', data=optimizer.learning_rate)  \n",
    "            hf.close()\n",
    "            \n",
    "            last_save = epoch #store the last time the val loss has decreased for early stop\n",
    "            \n",
    "        # Print loss values and training time (per epoch)\n",
    "        print('Epoch', epoch, '; Train_Loss', tloss_plot[epoch], \n",
    "              '; Val_Loss', vloss_plot[epoch],  '; Ratio', (vloss_plot[epoch])/(tloss_plot[epoch]))\n",
    "        print('Time per epoch', (time.time()-t)/N_check)\n",
    "        print('')\n",
    "        \n",
    "        t = time.time()\n",
    "        \n",
    "    if (epoch%20==0) and epoch != 0:    \n",
    "        #plot convergence of training and validation loss (to visualise convergence during training)\n",
    "        plt.title('MSE convergence')\n",
    "        plt.yscale('log')\n",
    "        plt.grid(True, axis=\"both\", which='both', ls=\"-\", alpha=0.3)\n",
    "        plt.plot(tloss_plot[np.nonzero(tloss_plot)], 'y', label='Train loss')\n",
    "        plt.plot(np.arange(np.nonzero(vloss_plot)[0].shape[0])*N_check, vloss_plot[np.nonzero(vloss_plot)], label='Val loss')\n",
    "        plt.xlabel('epochs')\n",
    "        plt.legend()    \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6caf216a22e1f8e1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualize error"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf8cee80fba0cc06"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "'''\n",
    "TODO: change path\n",
    "'''\n",
    "\n",
    "#load model for the test set\n",
    "path = './data/48_RE30_'+str(N_latent)\n",
    "# Load best model\n",
    "#how to load saved model\n",
    "a = [None]*N_parallel\n",
    "b = [None]*N_parallel\n",
    "for i in range(N_parallel):\n",
    "    a[i] = tf.keras.models.load_model(path + '/enc_mod'+str(ker_size[i])+'_'+str(N_latent)+'.h5', \n",
    "                                          custom_objects={\"PerPad2D\": PerPad2D})\n",
    "for i in range(N_parallel):\n",
    "    b[i] = tf.keras.models.load_model(path + '/dec_mod'+str(ker_size[i])+'_'+str(N_latent)+'.h5',\n",
    "                                          custom_objects={\"PerPad2D\": PerPad2D})"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c6c933e765a4b30"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#grid\n",
    "X       = np.linspace(0,2*np.pi,N_x) \n",
    "Y       = np.linspace(0,2*np.pi,N_y) \n",
    "XX      = np.meshgrid(X, Y, indexing='ij')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a343775d2a799f33"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#plot n snapshots and their reconstruction in the test set.\n",
    "n       = 5\n",
    "plt.rcParams[\"figure.figsize\"] = (15,4*n)\n",
    "plt.rcParams[\"font.size\"] = 20\n",
    "fig, ax = plt.subplots(n,3)\n",
    "\n",
    "start   = b_size*n_batches*skip+b_size*val_batches*skip #start after validation set\n",
    "\n",
    "for i in range(n):\n",
    "    \n",
    "    #truth\n",
    "    plt.subplot(n,3,i*3+1)\n",
    "    \n",
    "    skips = 50\n",
    "    \n",
    "    #snapshots to plot\n",
    "    u      = U[start+500+i*skips:start+501+i*skips].copy()      \n",
    "    vmax   = u.max()\n",
    "    vmin   = u.min()\n",
    "\n",
    "    CS0    = plt.contourf(XX[0], XX[1],u[0,:,:,0],\n",
    "                          levels=10,cmap='coolwarm',vmin=vmin, vmax=vmax)\n",
    "    cbar   = plt.colorbar()\n",
    "    cbar.set_label('$u_{\\mathrm{True}}$',labelpad=15)\n",
    "    CS     = plt.contour(XX[0], XX[1],u[0,:,:,0],\n",
    "                         levels=10,colors='black',linewidths=.5, linestyles='solid',\n",
    "                         vmin=vmin, vmax=vmax)\n",
    "    \n",
    "    #autoencoded\n",
    "    plt.subplot(n,3,i*3+2)\n",
    "\n",
    "    u_dec  = model(u,a,b)[1][0].numpy()\n",
    "    CS     = plt.contourf(XX[0],XX[1],u_dec[:,:,0],\n",
    "                        levels=10,cmap='coolwarm',vmin=vmin, vmax=vmax)\n",
    "    cbar   = plt.colorbar()\n",
    "    cbar.set_label('$u_{\\mathrm{Autoencoded}}$',labelpad=15)\n",
    "    CS     = plt.contour(XX[0], XX[1],u_dec[:,:,0],\n",
    "                         levels=10,colors='black',linewidths=.5, linestyles='solid',\n",
    "                         vmin=vmin, vmax=vmax)\n",
    "    \n",
    "    #error\n",
    "    plt.subplot(n,3,i*3+3)\n",
    "\n",
    "    u_err  = np.abs(u_dec-u[0])/(vmax-vmin)\n",
    "    print('NMAE: ', u_err[:,:,0].mean())\n",
    "    \n",
    "    CS     = plt.contourf(XX[0], XX[1],u_err[:,:,0],levels=10,cmap='coolwarm')\n",
    "    cbar   = plt.colorbar()\n",
    "    cbar.set_label('Relative Error',labelpad=15)\n",
    "    CS     = plt.contour(XX[0], XX[1],u_err[:,:,0],levels=10,colors='black',linewidths=.5, \n",
    "                         linestyles='solid')\n",
    "\n",
    "fig.tight_layout(pad=0.1)\n",
    "plt.savefig(path+'/Autoencoder_error.pdf')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1294525edbb68e86"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Save Encoded Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c9fd0737aa30b6ae"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "'''\n",
    "TODO: change path and parameters\n",
    "'''\n",
    "# save the encoded data for the ESN (too much memory used for GPU)\n",
    "N_pos     = 5000 #split in k interval of N_pos length needed to process long timeseries\n",
    "k         = 75\n",
    "transient = 10000\n",
    "N_len = k*N_pos\n",
    "fln      = '/data/ar994/Python/data/Kolmogorov/Kolmogorov_0.1_48_30.0_100100_32.h5'\n",
    "hf       = h5py.File(fln,'r')\n",
    "dt       = 0.1\n",
    "U        = np.array(hf.get('U')[transient:transient+N_len], dtype=np.float32)\n",
    "hf.close()\n",
    "\n",
    "N_x      = U.shape[1]\n",
    "N_y      = U.shape[2]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dcd58ade80fca017"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "'''\n",
    "TODO: change parameters\n",
    "'''\n",
    "\n",
    "Latents  = [18]\n",
    "Re       = 30\n",
    "\n",
    "for N_latent in Latents:\n",
    "    path = './data/48_RE30_'+str(N_latent)\n",
    "    a = [None]*N_parallel\n",
    "    b = [None]*N_parallel\n",
    "    for i in range(N_parallel):\n",
    "        a[i] = tf.keras.models.load_model(path + '/enc_mod'+str(ker_size[i])+'_'+str(N_latent)+'.h5', \n",
    "                                              custom_objects={\"PerPad2D\": PerPad2D})\n",
    "    for i in range(N_parallel):\n",
    "        b[i] = tf.keras.models.load_model(path + '/dec_mod'+str(ker_size[i])+'_'+str(N_latent)+'.h5',\n",
    "                                              custom_objects={\"PerPad2D\": PerPad2D})\n",
    "\n",
    "    N_1   = [3,3,N_latent//9]\n",
    "    U_enc = np.zeros((N_len, N_1[0], N_1[1], N_1[2]))\n",
    "    #encode all the data to provide time series in latent space for the ESN\n",
    "    for i in range(k):\n",
    "        U_enc[i*N_pos:(i+1)*N_pos]= model(U[i*N_pos:(i+1)*N_pos], a, b)[0]\n",
    "\n",
    "    fln = './data/48_Encoded_data_Re30_' \\\n",
    "                + str(N_latent) +'.h5'\n",
    "    hf = h5py.File(fln,'w')\n",
    "    hf.create_dataset('U_enc'      ,data=U_enc)  \n",
    "    hf.close()\n",
    "    print(fln)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "797d64a9be0d48d0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def gradient(U,dx,dy,n_splits):\n",
    "    '''Returns dissipation of U, done in n_splits'''\n",
    "    \n",
    "    shapes    = np.array(U.shape)\n",
    "    shapes[0] = shapes[0]//n_splits\n",
    "    dU_dx     = np.empty(shapes)\n",
    "    dU_dy     = np.empty(shapes)\n",
    "    D         = np.empty(shapes[0]*n_splits)\n",
    "    \n",
    "    for i in np.arange(n_splits):\n",
    "        \n",
    "        for j in range(shapes[1]):\n",
    "            dU_dx[:,j] = (U[i*shapes[0]:(i+1)*shapes[0],(j+1)%shapes[1]] - \\\n",
    "                          U[i*shapes[0]:(i+1)*shapes[0],j-1])/(2*dx)\n",
    "        for k in range(shapes[2]):\n",
    "            dU_dy[:,:,k] = (U[i*shapes[0]:(i+1)*shapes[0],:,(k+1)%shapes[2]] - \\\n",
    "                            U[i*shapes[0]:(i+1)*shapes[0],:,k-1])/(2*dy)\n",
    "            \n",
    "        D[i*shapes[0]:(i+1)*shapes[0]] =  np.mean(dU_dx**2+dU_dy**2,\n",
    "                                            axis=(1,2,3))/Re*4\n",
    "              \n",
    "    return D"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b7c645918c17745"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (15,4)\n",
    "plt.rcParams[\"font.size\"] = 20\n",
    "\n",
    "#plot average dissipation rate\n",
    "plt.subplot(121)\n",
    "leng      = 5000\n",
    "dx        = 2*np.pi/(N_x-1)\n",
    "DD        = gradient(U[-leng:],dx,dx,1) #true\n",
    "U_dec     = model(U[-leng:], a, b)[1]\n",
    "DD_enc    = gradient(U_dec,dx,dx,1) #autoencoded\n",
    "plt.plot(DD,'w')\n",
    "plt.plot(DD_enc,'r--')\n",
    "\n",
    "#plot error\n",
    "plt.subplot(122)\n",
    "plt.plot(np.abs(DD_enc-DD)/(DD.max()-DD.min()))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(np.mean(np.abs(DD_enc-DD)/(DD.max()-DD.min())))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a3d267c18b220b4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO: implement decoding\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c9d59000710bfb33"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
